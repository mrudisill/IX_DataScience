{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Evaluating Time Series </font>\n",
    "\n",
    "Great so we have some time series models - how do we tell if they're any good? In this module we'll explore how to evaluate our forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(16,5)})\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Building oour forecast </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the same SARIMAX model to predict airline passengers as we did before, however, instead of following the sklearn-like model syntax, we will use statsmodels api which is a way to use multiple kinds of statistical models with a similar syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = utils.load_airline_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = airlines[:'1957']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(airlines,          \n",
    "                          order=(0, 1, 1),           \n",
    "                          seasonal_order=(1, 1, 1, 12)\n",
    "                                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make 30 steps out of time \n",
    "train_up_to_step = len(airlines) - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=train_up_to_step, \n",
    "                              dynamic=False)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Time Series Evaluation Metrics </font> \n",
    "\n",
    "There are many metrics we can use to evaluate time series.\n",
    "\n",
    "A common metric for evaluating time series is the RMSE, and r-squared, $R^{2}$, which we have seen already.\n",
    "\n",
    "R2 is generally used only to validate the test set results. \n",
    "\n",
    "_Optional bit: The demonstration of why $R^{2}$ is beyond the scope here, but intuitive enough: a really complex model can overfit and get a high $R^{2}$, so if we use it as a metric to optimize when choosing the model we're incentivizing it to overfit._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate some test set results with R2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred.predicted_mean\n",
    "y_true = airlines.iloc[train_up_to_step::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> AIC  </font>\n",
    "\n",
    "As we mentioned, $R^{2}$ is limited when applied to the training set. This is where AIC is a better choice. \n",
    "\n",
    "AIC (Akaike information criterion) is a metric that will simutaneously measure how well the model fits the data, but will control for how complex the model is. If the model is very complex, the expectation oh how well it must fit the data will also go up. It is therefore useful for comparing models.  The lower the AIC, the better performance.\n",
    "\n",
    "If you (for some weird reason) feel compelled to calculate it by hand, [this post](https://stats.stackexchange.com/questions/87345/calculating-aic-by-hand-in-r?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) explains how to do so. Then again, it's sunny and beautiful outside, and Statsmodel has got your back. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's so useful (and hard to calculate), that statsmodels calculate it out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> MAPE </font>\n",
    "\n",
    "Mape stands for [Mean Absolute Percentage Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error), which is calculated similarly as the Mean Absolute Error, but using the percentage difference between prediction and true target values.\n",
    "\n",
    "It is usually presented as a percentage. For example, if the MAPE is 5, on average, the forecast is off by 5%. \n",
    "\n",
    "Its equation is:\n",
    "$$\n",
    "MAPE = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left |\\frac{\\hat{y}_i -y_i}{y_i}\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred- y_true) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Hyper parameter Optimization </font>\n",
    "\n",
    "Now, we have been using some very specific parameters: \n",
    "\n",
    "> p = 0  \n",
    "> d = 1  \n",
    "> q = 1  \n",
    "> P = 1  \n",
    "> D = 1  \n",
    "> Q = 1  \n",
    "> S = 12   \n",
    "\n",
    "I discovered these parameters by doing one of the following: \n",
    "> A) _~~developing a strong intuition about how statsmodels work and about the trends in the airline industry~~_   \n",
    "> B) throwing a hyper parameter optimizer at the problem and making myself a nice cup of tea while it ran. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SARIMAX parameter's space is fairly limited, we can do a very simple hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = P = D = Q = range(0, 2)   \n",
    "S = [7, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params_combinations = list(itertools.product(p, d, q, P, D, Q, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aic(series_, params):\n",
    "    # extract the params \n",
    "    p = params[0] \n",
    "    d = params[1] \n",
    "    q = params[2] \n",
    "    P = params[3]\n",
    "    D = params[4] \n",
    "    Q = params[5]\n",
    "    S = params[6]\n",
    "    \n",
    "    # fit a model with those params \n",
    "    model = sm.tsa.statespace.SARIMAX(series_,\n",
    "                                      order=(p, d, q),\n",
    "                                      seasonal_order=(P, D, Q, S),\n",
    "                                      enforce_stationarity=False,\n",
    "                                      enforce_invertibility=False)\n",
    "    \n",
    "    # fit the model\n",
    "    results = model.fit()\n",
    "    \n",
    "    # return the aic \n",
    "    return results.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "aic_scores = {}\n",
    "params_index = {}\n",
    "\n",
    "for i, param_set in enumerate(params_combinations):\n",
    "    aic = get_aic(airlines, param_set) \n",
    "    aic_scores[i] = aic\n",
    "    params_index[i] = param_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(params_index).T\n",
    "temp.columns = ['p', 'd', 'q', 'P', 'D', 'Q', 'S']\n",
    "temp['aic'] = pd.Series(aic_scores)\n",
    "temp.sort_values('aic').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <font color='#eb3483'> Using season and exogenous variables </font>\n",
    "\n",
    "So far, we have been using only the endogenous variable to create predictions.\n",
    "\n",
    "Also, the time series we used are ... kind of easy. Highly seasonal and periodic, eventhough the variance might increase over time. But they are easy. \n",
    "\n",
    "So, what about making things a \"little bit\" harder? Like, for example, predicting the US GDP Growth. If you want additional insights on each of the time series of this dataset, check this page at [thebalance.com](https://www.thebalance.com/components-of-gdp-explanation-formula-and-chart-3306015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/US_Production_Q_Data_Growth_Rates.csv')\n",
    "\n",
    "data.Year = pd.to_datetime(data.Year)\n",
    "\n",
    "data = data.set_index('Year')\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GDP Growth'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see any seasonality? Me neither. But you might notice that, from time to time, there is a big drop in GDP Growth (to negative values). That is related to economical cycles. Hmmm...cycles...maybe a cyclical component is present?\n",
    "\n",
    "**ALSO**, Remember the 2007 crisis? Well, it is no surprise that we had the biggest recession near ~2009. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all time series together to see if there is a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well...maybe it wasn't a good idea. Let's instead make several plots, one for each pair (GDP Growth, OTHER TIME SERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdp, other in itertools.product(['GDP Growth'], data.drop('GDP Growth', axis=1).columns): \n",
    "    data[[gdp, other]].plot()\n",
    "    sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection, we that Consumption Growth and Labor Growth follow GDP Growth very clearly (which makes sense, since consumption is one of the measures that compute GDP). So, one reasonable hypothesis is: those two time series are highly predictive for GDP Growth. \n",
    "\n",
    "Instead of trying to find the best model using grid search, we will explore the effect of each exogenous variable and model parameter in the forecast. \n",
    "\n",
    "First, let's prepare the train test split and, also, add some cyclical features for month and decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('GDP Growth', axis=1)\n",
    "X['month (cosine)'] = np.cos(2 * np.pi * X.index.month/ 12)\n",
    "X['month (sin)'] = np.sin(2 * np.pi * X.index.month/ 12)\n",
    "X['decade (cosine)'] = np.cos(2 * np.pi * (X.index.year % 10) / 10)\n",
    "X['decade (sine)'] = np.sin(2 * np.pi * (X.index.year % 10) / 10)\n",
    "\n",
    "y = data['GDP Growth']\n",
    "\n",
    "train_percentage = 0.5\n",
    "train_size = int(X.shape[0] * train_percentage)\n",
    "\n",
    "X_train, y_train = X.iloc[:train_size], y.iloc[:train_size]\n",
    "X_test, y_test = X.iloc[train_size:], y.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually create our own sklearn compatible sarima model for performing a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,  \n",
    "                 p, d, q, P, D, Q, S, \n",
    "                 trend, \n",
    "                 exog,\n",
    "                 forecast_steps=None, \n",
    "                 data_train=None,\n",
    "                 data_test=None):\n",
    "\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.q = q\n",
    "        self.trend = trend\n",
    "        self.P = P\n",
    "        self.D = D\n",
    "        self.Q = Q\n",
    "        self.S = S\n",
    "        self.exog = exog\n",
    "        self.data_train = data_train\n",
    "        self.data_test = data_test\n",
    "        self.forecast_steps = forecast_steps\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            if self.exog in self.data_train.columns:\n",
    "                exog_train = self.data_train[self.exog]  \n",
    "            else:\n",
    "                exog_train = None\n",
    "            self.model_ = sm.tsa.SARIMAX(y, \n",
    "                                 exog=exog_train, # the exogeneous variable\n",
    "                                 order=(self.p, self.d, self.q), # original arima p,d,q\n",
    "                                 seasonal_order=(self.P, self.D, self.Q, self.S) # Seasonal p,d,q\n",
    "                            )\n",
    "            self.results_ = self.model_.fit(maxiter=10, \n",
    "                                  trend=self.trend)\n",
    "                            \n",
    "        except:\n",
    "            self.results_ = (sm.tsa.SARIMAX(y, \n",
    "                                 order=(0,0,0), \n",
    "                                 seasonal_order=(0,0,0, 0))\n",
    "                             .fit(maxiter=1, \n",
    "                                  trend=self.trend)\n",
    "                            )\n",
    "    def predict(self, *args, **kwargs):\n",
    "        if self.exog in data.columns: \n",
    "            exog_test = (self\n",
    "                     .data_test[self.exog]\n",
    "                     .iloc[:self.forecast_steps]\n",
    "                     .values\n",
    "                     .reshape(self.forecast_steps, 1)\n",
    "                    )\n",
    "        else:\n",
    "            exog_test = None\n",
    "        return self.results_.get_forecast(\n",
    "            steps=self.forecast_steps, \n",
    "            exog=exog_test\n",
    "        ).predicted_mean\n",
    "    \n",
    "    def score(self, *args, **kwargs):\n",
    "        return -1. * self.results_.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SARIMAXWrapper(\n",
    "              data_train=X_train,\n",
    "              data_test=X_test,\n",
    "              exog=\"Investment Growth\", \n",
    "              p=1, d=1, q=1,\n",
    "              trend=\"nc\", \n",
    "              P=1, D=1, Q=1, S=1,\n",
    "            forecast_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(None, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import random\n",
    "\n",
    "search_dist = {\n",
    "    \"p\": randint(0,6),\n",
    "    \"d\": randint(0,6),\n",
    "    \"q\": randint(0,6),\n",
    "    \"P\": randint(0,6),\n",
    "    \"D\": randint(0,6),\n",
    "    \"Q\": randint(0,6),\n",
    "    \"S\": randint(0,13),\n",
    "    \"exog\": list(data.drop('GDP Growth', axis=1).columns) + [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RandomizedSearchCV(m, search_dist, n_iter=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the results of the search and find the optimal hyperparameters with exogeneus variable included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(r.cv_results_)\n",
    " .sort_values(by=\"mean_test_score\", ascending=False)\n",
    " .head(10)\n",
    " [[\n",
    "     \"param_p\",\n",
    "     \"param_d\",\n",
    "     \"param_q\",\n",
    "     \"param_P\",\n",
    "     \"param_D\",\n",
    "     \"param_Q\",\n",
    "     \"param_S\",\n",
    "     \"param_exog\",\n",
    "     \"mean_test_score\"\n",
    "  ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = sm.tsa.SARIMAX(y, \n",
    "                            exog=None, # the exogeneous variable\n",
    "                            order=(1, 1, 1), # original arima p,d,q\n",
    "                            seasonal_order=(3, 0, 0, 4) # Seasonal p,d,q\n",
    ")\n",
    "results = best_model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(series_, pred_):\n",
    "    \n",
    "    \"\"\" \n",
    "    Utility function to plot time series predictions with Confidence intervals\n",
    "    \"\"\"\n",
    "    mean_predictions_ = pred_.predicted_mean\n",
    "    pred_ci_ = pred_.conf_int()  \n",
    "    series_.plot(label='observed')\n",
    "    mean_predictions_.plot(label='predicted', \n",
    "                           alpha=.7)\n",
    "\n",
    "    sns.mpl.pyplot.fill_between(pred_ci_.index,\n",
    "                     pred_ci_['lower GDP Growth'],\n",
    "                     pred_ci_['upper GDP Growth'], \n",
    "                     color='k', \n",
    "                     alpha=.2)\n",
    "\n",
    "    sns.mpl.pyplot.ylim([-5, 15])\n",
    "    sns.mpl.pyplot.legend()\n",
    "    sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make 30 steps out of time \n",
    "train_up_to_step = len(y) - 10\n",
    "\n",
    "# remember the dynamic argument? Well, we'll use the first 30 steps to train\n",
    "pred = results.get_prediction(start=y.index.min(),  \n",
    "                              dynamic=train_up_to_step)                     \n",
    "    \n",
    "plot_predictions(series_=y, pred_=pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
